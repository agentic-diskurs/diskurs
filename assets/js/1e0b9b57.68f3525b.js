"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[350],{3066:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"llm_client","title":"Module: LLM Client","description":"class diskurs.llmclient.BaseOaiApiLLMClient(client, model, tokenizer, maxtokens, max_repeat=3)","source":"@site/docs/llm_client.md","sourceDirName":".","slug":"/llm_client","permalink":"/diskurs/docs/llm_client","draft":false,"unlisted":false,"editUrl":"https://github.com/agentic-diskurs/diskurs/edit/main/docs-site/docs/llm_client.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Welcome to Diskurs\u2019s documentation!","permalink":"/diskurs/docs/"},"next":{"title":"Module: Multistep Agent","permalink":"/diskurs/docs/multistep_agent"}}');var o=s(4848),i=s(8453);const r={},l="Module: LLM Client",c={},a=[{value:"<em>class</em> diskurs.llm_client.BaseOaiApiLLMClient(client, model, tokenizer, max_tokens, max_repeat=3)",id:"class-diskursllm_clientbaseoaiapillmclientclient-model-tokenizer-max_tokens-max_repeat3",level:3},{value:"<em>abstract classmethod</em> create(**kwargs)",id:"abstract-classmethod-createkwargs",level:4},{value:"send_request(body)",id:"send_requestbody",level:4},{value:"<em>static</em> format_tool_description_for_llm(tool)",id:"static-format_tool_description_for_llmtool",level:4},{value:"<em>static</em> format_message_for_llm(message)",id:"static-format_message_for_llmmessage",level:4},{value:"format_conversation_for_llm(conversation, tools=None)",id:"format_conversation_for_llmconversation-toolsnone",level:4},{value:"<em>classmethod</em> is_tool_call(completion)",id:"classmethod-is_tool_callcompletion",level:4},{value:"<em>classmethod</em> llm_response_to_chat_message(completion, agent_name, message_type)",id:"classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type",level:4},{value:"<em>classmethod</em> concatenate_user_prompt_with_llm_response(conversation, completion)",id:"classmethod-concatenate_user_prompt_with_llm_responseconversation-completion",level:4},{value:"count_tokens_in_conversation(messages)",id:"count_tokens_in_conversationmessages",level:4},{value:"count_tokens_recursively(value)",id:"count_tokens_recursivelyvalue",level:4},{value:"count_tokens(text)",id:"count_tokenstext",level:4},{value:"count_tokens_of_tool_descriptions(tool_descriptions)",id:"count_tokens_of_tool_descriptionstool_descriptions",level:4},{value:"truncate_chat_history(messages, n_tokens_tool_descriptions)",id:"truncate_chat_historymessages-n_tokens_tool_descriptions",level:4},{value:"generate(conversation, tools=None)",id:"generateconversation-toolsnone",level:4},{value:"<em>class</em> diskurs.llm_client.OpenAILLMClient(client, model, tokenizer, max_tokens, max_repeat=3)",id:"class-diskursllm_clientopenaillmclientclient-model-tokenizer-max_tokens-max_repeat3",level:3},{value:"<em>classmethod</em> create(**kwargs)",id:"classmethod-createkwargs",level:4},{value:"<em>classmethod</em> concatenate_user_prompt_with_llm_response(conversation, completion)",id:"classmethod-concatenate_user_prompt_with_llm_responseconversation-completion-1",level:4},{value:"count_tokens(text)",id:"count_tokenstext-1",level:4},{value:"count_tokens_in_conversation(messages)",id:"count_tokens_in_conversationmessages-1",level:4},{value:"count_tokens_of_tool_descriptions(tool_descriptions)",id:"count_tokens_of_tool_descriptionstool_descriptions-1",level:4},{value:"count_tokens_recursively(value)",id:"count_tokens_recursivelyvalue-1",level:4},{value:"format_conversation_for_llm(conversation, tools=None)",id:"format_conversation_for_llmconversation-toolsnone-1",level:4},{value:"<em>static</em> format_message_for_llm(message)",id:"static-format_message_for_llmmessage-1",level:4},{value:"<em>static</em> format_tool_description_for_llm(tool)",id:"static-format_tool_description_for_llmtool-1",level:4},{value:"generate(conversation, tools=None)",id:"generateconversation-toolsnone-1",level:4},{value:"<em>classmethod</em> is_tool_call(completion)",id:"classmethod-is_tool_callcompletion-1",level:4},{value:"<em>classmethod</em> llm_response_to_chat_message(completion, agent_name, message_type)",id:"classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type-1",level:4},{value:"send_request(body)",id:"send_requestbody-1",level:4},{value:"truncate_chat_history(messages, n_tokens_tool_descriptions)",id:"truncate_chat_historymessages-n_tokens_tool_descriptions-1",level:4}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-llm-client",children:"Module: LLM Client"})}),"\n",(0,o.jsxs)(n.h3,{id:"class-diskursllm_clientbaseoaiapillmclientclient-model-tokenizer-max_tokens-max_repeat3",children:[(0,o.jsx)(n.em,{children:"class"})," diskurs.llm_client.BaseOaiApiLLMClient(client, model, tokenizer, max_tokens, max_repeat=3)"]}),"\n",(0,o.jsxs)(n.p,{children:["Bases: ",(0,o.jsx)(n.a,{href:"/diskurs/docs/protocols#diskurs.protocols.LLMClient",children:(0,o.jsx)(n.code,{children:"LLMClient"})})]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"client"})," (",(0,o.jsx)(n.em,{children:"OpenAI"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model"})," (",(0,o.jsx)(n.em,{children:"str"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tokenizer"})," (",(0,o.jsx)(n.em,{children:"Callable"})," ",(0,o.jsx)(n.em,{children:"["})," *[*",(0,o.jsx)(n.em,{children:"str"})," ",(0,o.jsx)(n.em,{children:"]"})," ",(0,o.jsx)(n.em,{children:","})," ",(0,o.jsx)(n.em,{children:"int"})," ",(0,o.jsx)(n.em,{children:"]"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_tokens"})," (",(0,o.jsx)(n.em,{children:"int"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_repeat"})," (",(0,o.jsx)(n.em,{children:"int"}),")"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"abstract-classmethod-createkwargs",children:[(0,o.jsx)(n.em,{children:"abstract classmethod"})," create(**kwargs)"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"Self"})]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"send_requestbody",children:"send_request(body)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"ChatCompletion"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"body"})," (",(0,o.jsx)(n.em,{children:"dict"})," *[*",(0,o.jsx)(n.em,{children:"str"})," ",(0,o.jsx)(n.em,{children:","})," ",(0,o.jsx)(n.em,{children:"Any"})," ",(0,o.jsx)(n.em,{children:"]"}),")"]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"static-format_tool_description_for_llmtool",children:[(0,o.jsx)(n.em,{children:"static"})," format_tool_description_for_llm(tool)"]}),"\n",(0,o.jsxs)(n.p,{children:["Formats a ToolDescription object into a dictionary that can be sent to the LLM model.\n",":type"," tool: ",(0,o.jsx)(n.code,{children:"ToolDescription"}),"\n",":param"," tool: Tool description to be formatted\n:rtype: ",(0,o.jsx)(n.code,{children:"dict"}),"[",(0,o.jsx)(n.code,{children:"str"}),", ",(0,o.jsx)(n.code,{children:"Any"}),"]\n:return: JSON-serializable dictionary containing the tool data"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"tool"})," (",(0,o.jsx)(n.em,{children:"ToolDescription"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\ndict[str, ",(0,o.jsx)(n.em,{children:"Any"}),"]"]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"static-format_message_for_llmmessage",children:[(0,o.jsx)(n.em,{children:"static"})," format_message_for_llm(message)"]}),"\n",(0,o.jsx)(n.p,{children:"Formats a ChatMessage object into a dictionary that can be sent to the LLM model.\nUsed by the format_conversation_for_llm method to prepare individual messages for the LLM."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"message"})," (",(0,o.jsx)(n.code,{children:"ChatMessage"}),") \u2013 Message to be formatted"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"dict"}),"[",(0,o.jsx)(n.code,{children:"str"}),", ",(0,o.jsx)(n.code,{children:"str"}),"]"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nJSON-serializable dictionary containing the message data"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"format_conversation_for_llmconversation-toolsnone",children:"format_conversation_for_llm(conversation, tools=None)"}),"\n",(0,o.jsxs)(n.p,{children:["Formats the conversation object into a dictionary that can be sent to the LLM model.\nThis comprises the user prompt, chat history, and tool descriptions.\n",":type"," conversation: ",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.code,{children:"ImmutableConversation"})}),"\n",":param"," conversation: Contains all interactions so far\n",":type"," tools: ",(0,o.jsx)(n.code,{children:"Optional"}),"[",(0,o.jsx)(n.code,{children:"list"}),"[",(0,o.jsx)(n.code,{children:"ToolDescription"}),"]]\n",":param"," tools: The descriptions of all tools that the agent can use\n:rtype: ",(0,o.jsx)(n.code,{children:"dict"}),"[",(0,o.jsx)(n.code,{children:"str"}),", ",(0,o.jsx)(n.code,{children:"Any"}),"]\n:return: A JSON-serializable dictionary containing the conversation data ready for the LLM"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"conversation"})," (",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.em,{children:"ImmutableConversation"})}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tools"})," (",(0,o.jsx)(n.em,{children:"list"})," *[*",(0,o.jsx)(n.em,{children:"ToolDescription"})," ",(0,o.jsx)(n.em,{children:"]"}),"  ",(0,o.jsx)(n.em,{children:"|"})," ",(0,o.jsx)(n.em,{children:"None"}),")"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\ndict[str, ",(0,o.jsx)(n.em,{children:"Any"}),"]"]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"classmethod-is_tool_callcompletion",children:[(0,o.jsx)(n.em,{children:"classmethod"})," is_tool_call(completion)"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"bool"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"completion"})," (",(0,o.jsx)(n.em,{children:"ChatCompletion"}),")"]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type",children:[(0,o.jsx)(n.em,{children:"classmethod"})," llm_response_to_chat_message(completion, agent_name, message_type)"]}),"\n",(0,o.jsxs)(n.p,{children:["Converts the message returned by the LLM to a typed ChatMessage.\n",":type"," completion: ",(0,o.jsx)(n.code,{children:"ChatCompletion"}),"\n",":param"," completion: The response from the LLM model\n",":type"," agent_name: ",(0,o.jsx)(n.code,{children:"str"}),"\n",":param"," agent_name: The name of the agent whose question the completion is a response to\n",":type"," message_type: ",(0,o.jsx)(n.code,{children:"MessageType"}),"\n",":param"," message_type: The type of message to be created\n:rtype: ",(0,o.jsx)(n.code,{children:"ChatMessage"}),"\n:return: A ChatMessage object containing the structured response"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"completion"})," (",(0,o.jsx)(n.em,{children:"ChatCompletion"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"agent_name"})," (",(0,o.jsx)(n.em,{children:"str"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"message_type"})," (",(0,o.jsx)(n.em,{children:"MessageType"}),")"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.em,{children:"ChatMessage"})]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"classmethod-concatenate_user_prompt_with_llm_responseconversation-completion",children:[(0,o.jsx)(n.em,{children:"classmethod"})," concatenate_user_prompt_with_llm_response(conversation, completion)"]}),"\n",(0,o.jsx)(n.p,{children:"Creates a list of ChatMessages that combines the user prompt with the LLM response.\nEnsures a flat list, even if there are multiple messages in the user prompt (as is the case when\nmultiple tools are executed in a single pass)."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"conversation"})," (",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.code,{children:"ImmutableConversation"})}),") \u2013 the conversation containing the user prompt"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"completion"})," (",(0,o.jsx)(n.code,{children:"ChatCompletion"}),") \u2013 the response from the LLM model"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"list"}),"[",(0,o.jsx)(n.code,{children:"ChatMessage"}),"]"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nFlat list of ChatMessages containing the user prompt and LLM response"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"count_tokens_in_conversationmessages",children:"count_tokens_in_conversation(messages)"}),"\n",(0,o.jsx)(n.p,{children:"Count the number of tokens used by a list of messages i.e. chat history.\nThe implementation is based on OpenAI\u2019s token counting guidelines."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"int"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"messages"})," (",(0,o.jsx)(n.em,{children:"list"})," *[*",(0,o.jsx)(n.em,{children:"dict"})," ",(0,o.jsx)(n.em,{children:"]"}),")"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"count_tokens_recursivelyvalue",children:"count_tokens_recursively(value)"}),"\n",(0,o.jsx)(n.h4,{id:"count_tokenstext",children:"count_tokens(text)"}),"\n",(0,o.jsxs)(n.p,{children:["Counts the number of tokens in a text string.\n",":type"," text: ",(0,o.jsx)(n.code,{children:"str"}),"\n",":param"," text: The text string to tokenize.\n:rtype: ",(0,o.jsx)(n.code,{children:"int"}),"\n:return: The number of tokens in the text string."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"text"})," (",(0,o.jsx)(n.em,{children:"str"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\nint"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"count_tokens_of_tool_descriptionstool_descriptions",children:"count_tokens_of_tool_descriptions(tool_descriptions)"}),"\n",(0,o.jsxs)(n.p,{children:["Return the number of tokens used by the tool i.e. function description.\nUnfortunately, there\u2019s no documented way of counting those tokens, therefore we resort to best effort approach,\nhoping this implementation is a true upper bound.\nThe implementation is taken from:\n",(0,o.jsx)(n.a,{href:"https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/11",children:"https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/11"})]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"tool_descriptions"})," (",(0,o.jsx)(n.code,{children:"list"}),"[",(0,o.jsx)(n.code,{children:"dict"}),"[",(0,o.jsx)(n.code,{children:"str"}),", ",(0,o.jsx)(n.code,{children:"Any"}),"]]) \u2013 The description of all the tools"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"int"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nThe number of tokens used by the tools"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"truncate_chat_historymessages-n_tokens_tool_descriptions",children:"truncate_chat_history(messages, n_tokens_tool_descriptions)"}),"\n",(0,o.jsx)(n.p,{children:"Truncate the chat history to fit within the maximum token limit. The token limit is calculated as follows:\nWe retain the first two messages i.e. system prompt and initial user prompt and the last message.\nWe then truncate from left, removing messages from the chat history until the total token count is within the\nlimit. We also account for the token count of the tool descriptions."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"messages"})," \u2013 The list of messages in the conversation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"n_tokens_tool_descriptions"})," \u2013 The number of tokens used by the tool descriptions"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"list"}),"[",(0,o.jsx)(n.code,{children:"dict"}),"]"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nThe truncated chat history"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"generateconversation-toolsnone",children:"generate(conversation, tools=None)"}),"\n",(0,o.jsx)(n.p,{children:"Generates a response from the LLM model for the given conversation.\nHandles conversion from Conversation to LLM request format, sending the request to the LLM model,\nand converting the response back to a Conversation object."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"conversation"})," (",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.code,{children:"ImmutableConversation"})}),") \u2013 The conversation object containing the user prompt and chat history."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tools"})," (",(0,o.jsx)(n.code,{children:"Optional"}),"[",(0,o.jsx)(n.code,{children:"ToolDescription"}),"]) \u2013 Description of all the tools that the agent can use"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.code,{children:"ImmutableConversation"})})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nUpdated conversation object with the LLM response appended to the chat history."]}),"\n"]}),"\n",(0,o.jsxs)(n.h3,{id:"class-diskursllm_clientopenaillmclientclient-model-tokenizer-max_tokens-max_repeat3",children:[(0,o.jsx)(n.em,{children:"class"})," diskurs.llm_client.OpenAILLMClient(client, model, tokenizer, max_tokens, max_repeat=3)"]}),"\n",(0,o.jsxs)(n.p,{children:["Bases: ",(0,o.jsx)(n.a,{href:"#diskurs.llm_client.BaseOaiApiLLMClient",children:(0,o.jsx)(n.code,{children:"BaseOaiApiLLMClient"})})]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"client"})," (",(0,o.jsx)(n.em,{children:"OpenAI"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model"})," (",(0,o.jsx)(n.em,{children:"str"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tokenizer"})," (",(0,o.jsx)(n.em,{children:"Callable"})," ",(0,o.jsx)(n.em,{children:"["})," *[*",(0,o.jsx)(n.em,{children:"str"})," ",(0,o.jsx)(n.em,{children:"]"})," ",(0,o.jsx)(n.em,{children:","})," ",(0,o.jsx)(n.em,{children:"int"})," ",(0,o.jsx)(n.em,{children:"]"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_tokens"})," (",(0,o.jsx)(n.em,{children:"int"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_repeat"})," (",(0,o.jsx)(n.em,{children:"int"}),")"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"classmethod-createkwargs",children:[(0,o.jsx)(n.em,{children:"classmethod"})," create(**kwargs)"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"Self"})]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"classmethod-concatenate_user_prompt_with_llm_responseconversation-completion-1",children:[(0,o.jsx)(n.em,{children:"classmethod"})," concatenate_user_prompt_with_llm_response(conversation, completion)"]}),"\n",(0,o.jsx)(n.p,{children:"Creates a list of ChatMessages that combines the user prompt with the LLM response.\nEnsures a flat list, even if there are multiple messages in the user prompt (as is the case when\nmultiple tools are executed in a single pass)."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"conversation"})," (",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.code,{children:"ImmutableConversation"})}),") \u2013 the conversation containing the user prompt"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"completion"})," (",(0,o.jsx)(n.code,{children:"ChatCompletion"}),") \u2013 the response from the LLM model"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"list"}),"[",(0,o.jsx)(n.code,{children:"ChatMessage"}),"]"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nFlat list of ChatMessages containing the user prompt and LLM response"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"count_tokenstext-1",children:"count_tokens(text)"}),"\n",(0,o.jsxs)(n.p,{children:["Counts the number of tokens in a text string.\n",":type"," text: ",(0,o.jsx)(n.code,{children:"str"}),"\n",":param"," text: The text string to tokenize.\n:rtype: ",(0,o.jsx)(n.code,{children:"int"}),"\n:return: The number of tokens in the text string."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"text"})," (",(0,o.jsx)(n.em,{children:"str"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\nint"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"count_tokens_in_conversationmessages-1",children:"count_tokens_in_conversation(messages)"}),"\n",(0,o.jsx)(n.p,{children:"Count the number of tokens used by a list of messages i.e. chat history.\nThe implementation is based on OpenAI\u2019s token counting guidelines."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"int"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"messages"})," (",(0,o.jsx)(n.em,{children:"list"})," *[*",(0,o.jsx)(n.em,{children:"dict"})," ",(0,o.jsx)(n.em,{children:"]"}),")"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"count_tokens_of_tool_descriptionstool_descriptions-1",children:"count_tokens_of_tool_descriptions(tool_descriptions)"}),"\n",(0,o.jsxs)(n.p,{children:["Return the number of tokens used by the tool i.e. function description.\nUnfortunately, there\u2019s no documented way of counting those tokens, therefore we resort to best effort approach,\nhoping this implementation is a true upper bound.\nThe implementation is taken from:\n",(0,o.jsx)(n.a,{href:"https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/11",children:"https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/11"})]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"tool_descriptions"})," (",(0,o.jsx)(n.code,{children:"list"}),"[",(0,o.jsx)(n.code,{children:"dict"}),"[",(0,o.jsx)(n.code,{children:"str"}),", ",(0,o.jsx)(n.code,{children:"Any"}),"]]) \u2013 The description of all the tools"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"int"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nThe number of tokens used by the tools"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"count_tokens_recursivelyvalue-1",children:"count_tokens_recursively(value)"}),"\n",(0,o.jsx)(n.h4,{id:"format_conversation_for_llmconversation-toolsnone-1",children:"format_conversation_for_llm(conversation, tools=None)"}),"\n",(0,o.jsxs)(n.p,{children:["Formats the conversation object into a dictionary that can be sent to the LLM model.\nThis comprises the user prompt, chat history, and tool descriptions.\n",":type"," conversation: ",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.code,{children:"ImmutableConversation"})}),"\n",":param"," conversation: Contains all interactions so far\n",":type"," tools: ",(0,o.jsx)(n.code,{children:"Optional"}),"[",(0,o.jsx)(n.code,{children:"list"}),"[",(0,o.jsx)(n.code,{children:"ToolDescription"}),"]]\n",":param"," tools: The descriptions of all tools that the agent can use\n:rtype: ",(0,o.jsx)(n.code,{children:"dict"}),"[",(0,o.jsx)(n.code,{children:"str"}),", ",(0,o.jsx)(n.code,{children:"Any"}),"]\n:return: A JSON-serializable dictionary containing the conversation data ready for the LLM"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"conversation"})," (",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.em,{children:"ImmutableConversation"})}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tools"})," (",(0,o.jsx)(n.em,{children:"list"})," *[*",(0,o.jsx)(n.em,{children:"ToolDescription"})," ",(0,o.jsx)(n.em,{children:"]"}),"  ",(0,o.jsx)(n.em,{children:"|"})," ",(0,o.jsx)(n.em,{children:"None"}),")"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\ndict[str, ",(0,o.jsx)(n.em,{children:"Any"}),"]"]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"static-format_message_for_llmmessage-1",children:[(0,o.jsx)(n.em,{children:"static"})," format_message_for_llm(message)"]}),"\n",(0,o.jsx)(n.p,{children:"Formats a ChatMessage object into a dictionary that can be sent to the LLM model.\nUsed by the format_conversation_for_llm method to prepare individual messages for the LLM."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"message"})," (",(0,o.jsx)(n.code,{children:"ChatMessage"}),") \u2013 Message to be formatted"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"dict"}),"[",(0,o.jsx)(n.code,{children:"str"}),", ",(0,o.jsx)(n.code,{children:"str"}),"]"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nJSON-serializable dictionary containing the message data"]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"static-format_tool_description_for_llmtool-1",children:[(0,o.jsx)(n.em,{children:"static"})," format_tool_description_for_llm(tool)"]}),"\n",(0,o.jsxs)(n.p,{children:["Formats a ToolDescription object into a dictionary that can be sent to the LLM model.\n",":type"," tool: ",(0,o.jsx)(n.code,{children:"ToolDescription"}),"\n",":param"," tool: Tool description to be formatted\n:rtype: ",(0,o.jsx)(n.code,{children:"dict"}),"[",(0,o.jsx)(n.code,{children:"str"}),", ",(0,o.jsx)(n.code,{children:"Any"}),"]\n:return: JSON-serializable dictionary containing the tool data"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"tool"})," (",(0,o.jsx)(n.em,{children:"ToolDescription"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\ndict[str, ",(0,o.jsx)(n.em,{children:"Any"}),"]"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"generateconversation-toolsnone-1",children:"generate(conversation, tools=None)"}),"\n",(0,o.jsx)(n.p,{children:"Generates a response from the LLM model for the given conversation.\nHandles conversion from Conversation to LLM request format, sending the request to the LLM model,\nand converting the response back to a Conversation object."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"conversation"})," (",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.code,{children:"ImmutableConversation"})}),") \u2013 The conversation object containing the user prompt and chat history."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tools"})," (",(0,o.jsx)(n.code,{children:"Optional"}),"[",(0,o.jsx)(n.code,{children:"ToolDescription"}),"]) \u2013 Description of all the tools that the agent can use"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.a,{href:"/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation",children:(0,o.jsx)(n.code,{children:"ImmutableConversation"})})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nUpdated conversation object with the LLM response appended to the chat history."]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"classmethod-is_tool_callcompletion-1",children:[(0,o.jsx)(n.em,{children:"classmethod"})," is_tool_call(completion)"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"bool"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"completion"})," (",(0,o.jsx)(n.em,{children:"ChatCompletion"}),")"]}),"\n"]}),"\n",(0,o.jsxs)(n.h4,{id:"classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type-1",children:[(0,o.jsx)(n.em,{children:"classmethod"})," llm_response_to_chat_message(completion, agent_name, message_type)"]}),"\n",(0,o.jsxs)(n.p,{children:["Converts the message returned by the LLM to a typed ChatMessage.\n",":type"," completion: ",(0,o.jsx)(n.code,{children:"ChatCompletion"}),"\n",":param"," completion: The response from the LLM model\n",":type"," agent_name: ",(0,o.jsx)(n.code,{children:"str"}),"\n",":param"," agent_name: The name of the agent whose question the completion is a response to\n",":type"," message_type: ",(0,o.jsx)(n.code,{children:"MessageType"}),"\n",":param"," message_type: The type of message to be created\n:rtype: ",(0,o.jsx)(n.code,{children:"ChatMessage"}),"\n:return: A ChatMessage object containing the structured response"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"completion"})," (",(0,o.jsx)(n.em,{children:"ChatCompletion"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"agent_name"})," (",(0,o.jsx)(n.em,{children:"str"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"message_type"})," (",(0,o.jsx)(n.em,{children:"MessageType"}),")"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.em,{children:"ChatMessage"})]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"send_requestbody-1",children:"send_request(body)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"ChatCompletion"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsx)(n.strong,{children:"body"})," (",(0,o.jsx)(n.em,{children:"dict"})," *[*",(0,o.jsx)(n.em,{children:"str"})," ",(0,o.jsx)(n.em,{children:","})," ",(0,o.jsx)(n.em,{children:"Any"})," ",(0,o.jsx)(n.em,{children:"]"}),")"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"truncate_chat_historymessages-n_tokens_tool_descriptions-1",children:"truncate_chat_history(messages, n_tokens_tool_descriptions)"}),"\n",(0,o.jsx)(n.p,{children:"Truncate the chat history to fit within the maximum token limit. The token limit is calculated as follows:\nWe retain the first two messages i.e. system prompt and initial user prompt and the last message.\nWe then truncate from left, removing messages from the chat history until the total token count is within the\nlimit. We also account for the token count of the tool descriptions."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parameters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"messages"})," \u2013 The list of messages in the conversation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"n_tokens_tool_descriptions"})," \u2013 The number of tokens used by the tool descriptions"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Return type:"}),"\n",(0,o.jsx)(n.code,{children:"list"}),"[",(0,o.jsx)(n.code,{children:"dict"}),"]"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Returns:"}),"\nThe truncated chat history"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var t=s(6540);const o={},i=t.createContext(o);function r(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);