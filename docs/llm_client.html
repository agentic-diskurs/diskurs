<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-llm_client" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.1">
<title data-rh="true">Module: LLM Client | Diskurs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://agentic-diskurs.github.io/diskurs/img/diskurs-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://agentic-diskurs.github.io/diskurs/img/diskurs-social-card.jpg"><meta data-rh="true" property="og:url" content="https://agentic-diskurs.github.io/diskurs/docs/llm_client"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Module: LLM Client | Diskurs"><meta data-rh="true" name="description" content="class diskurs.llmclient.BaseOaiApiLLMClient(client, model, tokenizer, maxtokens, max_repeat=3)"><meta data-rh="true" property="og:description" content="class diskurs.llmclient.BaseOaiApiLLMClient(client, model, tokenizer, maxtokens, max_repeat=3)"><link data-rh="true" rel="icon" href="/diskurs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://agentic-diskurs.github.io/diskurs/docs/llm_client"><link data-rh="true" rel="alternate" href="https://agentic-diskurs.github.io/diskurs/docs/llm_client" hreflang="en"><link data-rh="true" rel="alternate" href="https://agentic-diskurs.github.io/diskurs/docs/llm_client" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/diskurs/blog/rss.xml" title="Diskurs RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/diskurs/blog/atom.xml" title="Diskurs Atom Feed"><link rel="stylesheet" href="/diskurs/assets/css/styles.7a6aa77d.css">
<script src="/diskurs/assets/js/runtime~main.d886a170.js" defer="defer"></script>
<script src="/diskurs/assets/js/main.edf29e07.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/diskurs/"><div class="navbar__logo"><img src="/diskurs/img/logo.svg" alt="Diskurs Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/diskurs/img/logo.svg" alt="Diskurs Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Diskurs</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/diskurs/docs/intro">Documentation</a><a class="navbar__item navbar__link" href="/diskurs/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/agentic-diskurs/diskurs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/diskurs/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Expand sidebar category &#x27;Tutorial - Basics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/diskurs/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Expand sidebar category &#x27;Tutorial - Extras&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs/conductor_agent">Module: Conductor Agent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs/dispatcher">Module: Dispatcher</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs/filesystem_conversation_store">Module: Filesystem Conversation Store</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs/heuristic_agent">Module: Heuristic Agent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs/immutable_conversation">Module: Immutable Conversation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs">Welcome to Diskursâ€™s documentation!</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/diskurs/docs/llm_client">Module: LLM Client</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs/multistep_agent">Module: Multistep Agent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs/prompt">Module: Prompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/diskurs/docs/protocols">Protocols</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/diskurs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Module: LLM Client</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Module: LLM Client</h1></header>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-diskursllm_clientbaseoaiapillmclientclient-model-tokenizer-max_tokens-max_repeat3"><em>class</em> diskurs.llm_client.BaseOaiApiLLMClient(client, model, tokenizer, max_tokens, max_repeat=3)<a href="#class-diskursllm_clientbaseoaiapillmclientclient-model-tokenizer-max_tokens-max_repeat3" class="hash-link" aria-label="Direct link to class-diskursllm_clientbaseoaiapillmclientclient-model-tokenizer-max_tokens-max_repeat3" title="Direct link to class-diskursllm_clientbaseoaiapillmclientclient-model-tokenizer-max_tokens-max_repeat3">â€‹</a></h3>
<p>Bases: <a href="/diskurs/docs/protocols#diskurs.protocols.LLMClient"><code>LLMClient</code></a></p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>client</strong> (<em>OpenAI</em>)</li>
<li><strong>model</strong> (<em>str</em>)</li>
<li><strong>tokenizer</strong> (<em>Callable</em> <em>[</em> *[*<em>str</em> <em>]</em> <em>,</em> <em>int</em> <em>]</em>)</li>
<li><strong>max_tokens</strong> (<em>int</em>)</li>
<li><strong>max_repeat</strong> (<em>int</em>)</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="abstract-classmethod-createkwargs"><em>abstract classmethod</em> create(**kwargs)<a href="#abstract-classmethod-createkwargs" class="hash-link" aria-label="Direct link to abstract-classmethod-createkwargs" title="Direct link to abstract-classmethod-createkwargs">â€‹</a></h4>
<ul>
<li><strong>Return type:</strong>
<code>Self</code></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="send_requestbody">send_request(body)<a href="#send_requestbody" class="hash-link" aria-label="Direct link to send_request(body)" title="Direct link to send_request(body)">â€‹</a></h4>
<ul>
<li><strong>Return type:</strong>
<code>ChatCompletion</code></li>
<li><strong>Parameters:</strong>
<strong>body</strong> (<em>dict</em> *[*<em>str</em> <em>,</em> <em>Any</em> <em>]</em>)</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="static-format_tool_description_for_llmtool"><em>static</em> format_tool_description_for_llm(tool)<a href="#static-format_tool_description_for_llmtool" class="hash-link" aria-label="Direct link to static-format_tool_description_for_llmtool" title="Direct link to static-format_tool_description_for_llmtool">â€‹</a></h4>
<p>Formats a ToolDescription object into a dictionary that can be sent to the LLM model.
<!-- -->:type<!-- --> tool: <code>ToolDescription</code>
<!-- -->:param<!-- --> tool: Tool description to be formatted
:rtype: <code>dict</code>[<code>str</code>, <code>Any</code>]
:return: JSON-serializable dictionary containing the tool data</p>
<ul>
<li><strong>Parameters:</strong>
<strong>tool</strong> (<em>ToolDescription</em>)</li>
<li><strong>Return type:</strong>
dict[str, <em>Any</em>]</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="static-format_message_for_llmmessage"><em>static</em> format_message_for_llm(message)<a href="#static-format_message_for_llmmessage" class="hash-link" aria-label="Direct link to static-format_message_for_llmmessage" title="Direct link to static-format_message_for_llmmessage">â€‹</a></h4>
<p>Formats a ChatMessage object into a dictionary that can be sent to the LLM model.
Used by the format_conversation_for_llm method to prepare individual messages for the LLM.</p>
<ul>
<li><strong>Parameters:</strong>
<strong>message</strong> (<code>ChatMessage</code>) â€“ Message to be formatted</li>
<li><strong>Return type:</strong>
<code>dict</code>[<code>str</code>, <code>str</code>]</li>
<li><strong>Returns:</strong>
JSON-serializable dictionary containing the message data</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="format_conversation_for_llmconversation-toolsnone">format_conversation_for_llm(conversation, tools=None)<a href="#format_conversation_for_llmconversation-toolsnone" class="hash-link" aria-label="Direct link to format_conversation_for_llm(conversation, tools=None)" title="Direct link to format_conversation_for_llm(conversation, tools=None)">â€‹</a></h4>
<p>Formats the conversation object into a dictionary that can be sent to the LLM model.
This comprises the user prompt, chat history, and tool descriptions.
<!-- -->:type<!-- --> conversation: <a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><code>ImmutableConversation</code></a>
<!-- -->:param<!-- --> conversation: Contains all interactions so far
<!-- -->:type<!-- --> tools: <code>Optional</code>[<code>list</code>[<code>ToolDescription</code>]]
<!-- -->:param<!-- --> tools: The descriptions of all tools that the agent can use
:rtype: <code>dict</code>[<code>str</code>, <code>Any</code>]
:return: A JSON-serializable dictionary containing the conversation data ready for the LLM</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>conversation</strong> (<a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><em>ImmutableConversation</em></a>)</li>
<li><strong>tools</strong> (<em>list</em> *[*<em>ToolDescription</em> <em>]</em>  <em>|</em> <em>None</em>)</li>
</ul>
</li>
<li><strong>Return type:</strong>
dict[str, <em>Any</em>]</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-is_tool_callcompletion"><em>classmethod</em> is_tool_call(completion)<a href="#classmethod-is_tool_callcompletion" class="hash-link" aria-label="Direct link to classmethod-is_tool_callcompletion" title="Direct link to classmethod-is_tool_callcompletion">â€‹</a></h4>
<ul>
<li><strong>Return type:</strong>
<code>bool</code></li>
<li><strong>Parameters:</strong>
<strong>completion</strong> (<em>ChatCompletion</em>)</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type"><em>classmethod</em> llm_response_to_chat_message(completion, agent_name, message_type)<a href="#classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type" class="hash-link" aria-label="Direct link to classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type" title="Direct link to classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type">â€‹</a></h4>
<p>Converts the message returned by the LLM to a typed ChatMessage.
<!-- -->:type<!-- --> completion: <code>ChatCompletion</code>
<!-- -->:param<!-- --> completion: The response from the LLM model
<!-- -->:type<!-- --> agent_name: <code>str</code>
<!-- -->:param<!-- --> agent_name: The name of the agent whose question the completion is a response to
<!-- -->:type<!-- --> message_type: <code>MessageType</code>
<!-- -->:param<!-- --> message_type: The type of message to be created
:rtype: <code>ChatMessage</code>
:return: A ChatMessage object containing the structured response</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>completion</strong> (<em>ChatCompletion</em>)</li>
<li><strong>agent_name</strong> (<em>str</em>)</li>
<li><strong>message_type</strong> (<em>MessageType</em>)</li>
</ul>
</li>
<li><strong>Return type:</strong>
<em>ChatMessage</em></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-concatenate_user_prompt_with_llm_responseconversation-completion"><em>classmethod</em> concatenate_user_prompt_with_llm_response(conversation, completion)<a href="#classmethod-concatenate_user_prompt_with_llm_responseconversation-completion" class="hash-link" aria-label="Direct link to classmethod-concatenate_user_prompt_with_llm_responseconversation-completion" title="Direct link to classmethod-concatenate_user_prompt_with_llm_responseconversation-completion">â€‹</a></h4>
<p>Creates a list of ChatMessages that combines the user prompt with the LLM response.
Ensures a flat list, even if there are multiple messages in the user prompt (as is the case when
multiple tools are executed in a single pass).</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>conversation</strong> (<a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><code>ImmutableConversation</code></a>) â€“ the conversation containing the user prompt</li>
<li><strong>completion</strong> (<code>ChatCompletion</code>) â€“ the response from the LLM model</li>
</ul>
</li>
<li><strong>Return type:</strong>
<code>list</code>[<code>ChatMessage</code>]</li>
<li><strong>Returns:</strong>
Flat list of ChatMessages containing the user prompt and LLM response</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="count_tokens_in_conversationmessages">count_tokens_in_conversation(messages)<a href="#count_tokens_in_conversationmessages" class="hash-link" aria-label="Direct link to count_tokens_in_conversation(messages)" title="Direct link to count_tokens_in_conversation(messages)">â€‹</a></h4>
<p>Count the number of tokens used by a list of messages i.e. chat history.
The implementation is based on OpenAIâ€™s token counting guidelines.</p>
<ul>
<li><strong>Return type:</strong>
<code>int</code></li>
<li><strong>Parameters:</strong>
<strong>messages</strong> (<em>list</em> *[*<em>dict</em> <em>]</em>)</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="count_tokens_recursivelyvalue">count_tokens_recursively(value)<a href="#count_tokens_recursivelyvalue" class="hash-link" aria-label="Direct link to count_tokens_recursively(value)" title="Direct link to count_tokens_recursively(value)">â€‹</a></h4>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="count_tokenstext">count_tokens(text)<a href="#count_tokenstext" class="hash-link" aria-label="Direct link to count_tokens(text)" title="Direct link to count_tokens(text)">â€‹</a></h4>
<p>Counts the number of tokens in a text string.
<!-- -->:type<!-- --> text: <code>str</code>
<!-- -->:param<!-- --> text: The text string to tokenize.
:rtype: <code>int</code>
:return: The number of tokens in the text string.</p>
<ul>
<li><strong>Parameters:</strong>
<strong>text</strong> (<em>str</em>)</li>
<li><strong>Return type:</strong>
int</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="count_tokens_of_tool_descriptionstool_descriptions">count_tokens_of_tool_descriptions(tool_descriptions)<a href="#count_tokens_of_tool_descriptionstool_descriptions" class="hash-link" aria-label="Direct link to count_tokens_of_tool_descriptions(tool_descriptions)" title="Direct link to count_tokens_of_tool_descriptions(tool_descriptions)">â€‹</a></h4>
<p>Return the number of tokens used by the tool i.e. function description.
Unfortunately, thereâ€™s no documented way of counting those tokens, therefore we resort to best effort approach,
hoping this implementation is a true upper bound.
The implementation is taken from:
<a href="https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/11" target="_blank" rel="noopener noreferrer">https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/11</a></p>
<ul>
<li><strong>Parameters:</strong>
<strong>tool_descriptions</strong> (<code>list</code>[<code>dict</code>[<code>str</code>, <code>Any</code>]]) â€“ The description of all the tools</li>
<li><strong>Return type:</strong>
<code>int</code></li>
<li><strong>Returns:</strong>
The number of tokens used by the tools</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="truncate_chat_historymessages-n_tokens_tool_descriptions">truncate_chat_history(messages, n_tokens_tool_descriptions)<a href="#truncate_chat_historymessages-n_tokens_tool_descriptions" class="hash-link" aria-label="Direct link to truncate_chat_history(messages, n_tokens_tool_descriptions)" title="Direct link to truncate_chat_history(messages, n_tokens_tool_descriptions)">â€‹</a></h4>
<p>Truncate the chat history to fit within the maximum token limit. The token limit is calculated as follows:
We retain the first two messages i.e. system prompt and initial user prompt and the last message.
We then truncate from left, removing messages from the chat history until the total token count is within the
limit. We also account for the token count of the tool descriptions.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>messages</strong> â€“ The list of messages in the conversation</li>
<li><strong>n_tokens_tool_descriptions</strong> â€“ The number of tokens used by the tool descriptions</li>
</ul>
</li>
<li><strong>Return type:</strong>
<code>list</code>[<code>dict</code>]</li>
<li><strong>Returns:</strong>
The truncated chat history</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="generateconversation-toolsnone">generate(conversation, tools=None)<a href="#generateconversation-toolsnone" class="hash-link" aria-label="Direct link to generate(conversation, tools=None)" title="Direct link to generate(conversation, tools=None)">â€‹</a></h4>
<p>Generates a response from the LLM model for the given conversation.
Handles conversion from Conversation to LLM request format, sending the request to the LLM model,
and converting the response back to a Conversation object.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>conversation</strong> (<a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><code>ImmutableConversation</code></a>) â€“ The conversation object containing the user prompt and chat history.</li>
<li><strong>tools</strong> (<code>Optional</code>[<code>ToolDescription</code>]) â€“ Description of all the tools that the agent can use</li>
</ul>
</li>
<li><strong>Return type:</strong>
<a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><code>ImmutableConversation</code></a></li>
<li><strong>Returns:</strong>
Updated conversation object with the LLM response appended to the chat history.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-diskursllm_clientopenaillmclientclient-model-tokenizer-max_tokens-max_repeat3"><em>class</em> diskurs.llm_client.OpenAILLMClient(client, model, tokenizer, max_tokens, max_repeat=3)<a href="#class-diskursllm_clientopenaillmclientclient-model-tokenizer-max_tokens-max_repeat3" class="hash-link" aria-label="Direct link to class-diskursllm_clientopenaillmclientclient-model-tokenizer-max_tokens-max_repeat3" title="Direct link to class-diskursllm_clientopenaillmclientclient-model-tokenizer-max_tokens-max_repeat3">â€‹</a></h3>
<p>Bases: <a href="#diskurs.llm_client.BaseOaiApiLLMClient"><code>BaseOaiApiLLMClient</code></a></p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>client</strong> (<em>OpenAI</em>)</li>
<li><strong>model</strong> (<em>str</em>)</li>
<li><strong>tokenizer</strong> (<em>Callable</em> <em>[</em> *[*<em>str</em> <em>]</em> <em>,</em> <em>int</em> <em>]</em>)</li>
<li><strong>max_tokens</strong> (<em>int</em>)</li>
<li><strong>max_repeat</strong> (<em>int</em>)</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-createkwargs"><em>classmethod</em> create(**kwargs)<a href="#classmethod-createkwargs" class="hash-link" aria-label="Direct link to classmethod-createkwargs" title="Direct link to classmethod-createkwargs">â€‹</a></h4>
<ul>
<li><strong>Return type:</strong>
<code>Self</code></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-concatenate_user_prompt_with_llm_responseconversation-completion-1"><em>classmethod</em> concatenate_user_prompt_with_llm_response(conversation, completion)<a href="#classmethod-concatenate_user_prompt_with_llm_responseconversation-completion-1" class="hash-link" aria-label="Direct link to classmethod-concatenate_user_prompt_with_llm_responseconversation-completion-1" title="Direct link to classmethod-concatenate_user_prompt_with_llm_responseconversation-completion-1">â€‹</a></h4>
<p>Creates a list of ChatMessages that combines the user prompt with the LLM response.
Ensures a flat list, even if there are multiple messages in the user prompt (as is the case when
multiple tools are executed in a single pass).</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>conversation</strong> (<a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><code>ImmutableConversation</code></a>) â€“ the conversation containing the user prompt</li>
<li><strong>completion</strong> (<code>ChatCompletion</code>) â€“ the response from the LLM model</li>
</ul>
</li>
<li><strong>Return type:</strong>
<code>list</code>[<code>ChatMessage</code>]</li>
<li><strong>Returns:</strong>
Flat list of ChatMessages containing the user prompt and LLM response</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="count_tokenstext-1">count_tokens(text)<a href="#count_tokenstext-1" class="hash-link" aria-label="Direct link to count_tokens(text)" title="Direct link to count_tokens(text)">â€‹</a></h4>
<p>Counts the number of tokens in a text string.
<!-- -->:type<!-- --> text: <code>str</code>
<!-- -->:param<!-- --> text: The text string to tokenize.
:rtype: <code>int</code>
:return: The number of tokens in the text string.</p>
<ul>
<li><strong>Parameters:</strong>
<strong>text</strong> (<em>str</em>)</li>
<li><strong>Return type:</strong>
int</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="count_tokens_in_conversationmessages-1">count_tokens_in_conversation(messages)<a href="#count_tokens_in_conversationmessages-1" class="hash-link" aria-label="Direct link to count_tokens_in_conversation(messages)" title="Direct link to count_tokens_in_conversation(messages)">â€‹</a></h4>
<p>Count the number of tokens used by a list of messages i.e. chat history.
The implementation is based on OpenAIâ€™s token counting guidelines.</p>
<ul>
<li><strong>Return type:</strong>
<code>int</code></li>
<li><strong>Parameters:</strong>
<strong>messages</strong> (<em>list</em> *[*<em>dict</em> <em>]</em>)</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="count_tokens_of_tool_descriptionstool_descriptions-1">count_tokens_of_tool_descriptions(tool_descriptions)<a href="#count_tokens_of_tool_descriptionstool_descriptions-1" class="hash-link" aria-label="Direct link to count_tokens_of_tool_descriptions(tool_descriptions)" title="Direct link to count_tokens_of_tool_descriptions(tool_descriptions)">â€‹</a></h4>
<p>Return the number of tokens used by the tool i.e. function description.
Unfortunately, thereâ€™s no documented way of counting those tokens, therefore we resort to best effort approach,
hoping this implementation is a true upper bound.
The implementation is taken from:
<a href="https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/11" target="_blank" rel="noopener noreferrer">https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/11</a></p>
<ul>
<li><strong>Parameters:</strong>
<strong>tool_descriptions</strong> (<code>list</code>[<code>dict</code>[<code>str</code>, <code>Any</code>]]) â€“ The description of all the tools</li>
<li><strong>Return type:</strong>
<code>int</code></li>
<li><strong>Returns:</strong>
The number of tokens used by the tools</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="count_tokens_recursivelyvalue-1">count_tokens_recursively(value)<a href="#count_tokens_recursivelyvalue-1" class="hash-link" aria-label="Direct link to count_tokens_recursively(value)" title="Direct link to count_tokens_recursively(value)">â€‹</a></h4>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="format_conversation_for_llmconversation-toolsnone-1">format_conversation_for_llm(conversation, tools=None)<a href="#format_conversation_for_llmconversation-toolsnone-1" class="hash-link" aria-label="Direct link to format_conversation_for_llm(conversation, tools=None)" title="Direct link to format_conversation_for_llm(conversation, tools=None)">â€‹</a></h4>
<p>Formats the conversation object into a dictionary that can be sent to the LLM model.
This comprises the user prompt, chat history, and tool descriptions.
<!-- -->:type<!-- --> conversation: <a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><code>ImmutableConversation</code></a>
<!-- -->:param<!-- --> conversation: Contains all interactions so far
<!-- -->:type<!-- --> tools: <code>Optional</code>[<code>list</code>[<code>ToolDescription</code>]]
<!-- -->:param<!-- --> tools: The descriptions of all tools that the agent can use
:rtype: <code>dict</code>[<code>str</code>, <code>Any</code>]
:return: A JSON-serializable dictionary containing the conversation data ready for the LLM</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>conversation</strong> (<a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><em>ImmutableConversation</em></a>)</li>
<li><strong>tools</strong> (<em>list</em> *[*<em>ToolDescription</em> <em>]</em>  <em>|</em> <em>None</em>)</li>
</ul>
</li>
<li><strong>Return type:</strong>
dict[str, <em>Any</em>]</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="static-format_message_for_llmmessage-1"><em>static</em> format_message_for_llm(message)<a href="#static-format_message_for_llmmessage-1" class="hash-link" aria-label="Direct link to static-format_message_for_llmmessage-1" title="Direct link to static-format_message_for_llmmessage-1">â€‹</a></h4>
<p>Formats a ChatMessage object into a dictionary that can be sent to the LLM model.
Used by the format_conversation_for_llm method to prepare individual messages for the LLM.</p>
<ul>
<li><strong>Parameters:</strong>
<strong>message</strong> (<code>ChatMessage</code>) â€“ Message to be formatted</li>
<li><strong>Return type:</strong>
<code>dict</code>[<code>str</code>, <code>str</code>]</li>
<li><strong>Returns:</strong>
JSON-serializable dictionary containing the message data</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="static-format_tool_description_for_llmtool-1"><em>static</em> format_tool_description_for_llm(tool)<a href="#static-format_tool_description_for_llmtool-1" class="hash-link" aria-label="Direct link to static-format_tool_description_for_llmtool-1" title="Direct link to static-format_tool_description_for_llmtool-1">â€‹</a></h4>
<p>Formats a ToolDescription object into a dictionary that can be sent to the LLM model.
<!-- -->:type<!-- --> tool: <code>ToolDescription</code>
<!-- -->:param<!-- --> tool: Tool description to be formatted
:rtype: <code>dict</code>[<code>str</code>, <code>Any</code>]
:return: JSON-serializable dictionary containing the tool data</p>
<ul>
<li><strong>Parameters:</strong>
<strong>tool</strong> (<em>ToolDescription</em>)</li>
<li><strong>Return type:</strong>
dict[str, <em>Any</em>]</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="generateconversation-toolsnone-1">generate(conversation, tools=None)<a href="#generateconversation-toolsnone-1" class="hash-link" aria-label="Direct link to generate(conversation, tools=None)" title="Direct link to generate(conversation, tools=None)">â€‹</a></h4>
<p>Generates a response from the LLM model for the given conversation.
Handles conversion from Conversation to LLM request format, sending the request to the LLM model,
and converting the response back to a Conversation object.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>conversation</strong> (<a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><code>ImmutableConversation</code></a>) â€“ The conversation object containing the user prompt and chat history.</li>
<li><strong>tools</strong> (<code>Optional</code>[<code>ToolDescription</code>]) â€“ Description of all the tools that the agent can use</li>
</ul>
</li>
<li><strong>Return type:</strong>
<a href="/diskurs/docs/immutable_conversation#diskurs.immutable_conversation.ImmutableConversation"><code>ImmutableConversation</code></a></li>
<li><strong>Returns:</strong>
Updated conversation object with the LLM response appended to the chat history.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-is_tool_callcompletion-1"><em>classmethod</em> is_tool_call(completion)<a href="#classmethod-is_tool_callcompletion-1" class="hash-link" aria-label="Direct link to classmethod-is_tool_callcompletion-1" title="Direct link to classmethod-is_tool_callcompletion-1">â€‹</a></h4>
<ul>
<li><strong>Return type:</strong>
<code>bool</code></li>
<li><strong>Parameters:</strong>
<strong>completion</strong> (<em>ChatCompletion</em>)</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type-1"><em>classmethod</em> llm_response_to_chat_message(completion, agent_name, message_type)<a href="#classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type-1" class="hash-link" aria-label="Direct link to classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type-1" title="Direct link to classmethod-llm_response_to_chat_messagecompletion-agent_name-message_type-1">â€‹</a></h4>
<p>Converts the message returned by the LLM to a typed ChatMessage.
<!-- -->:type<!-- --> completion: <code>ChatCompletion</code>
<!-- -->:param<!-- --> completion: The response from the LLM model
<!-- -->:type<!-- --> agent_name: <code>str</code>
<!-- -->:param<!-- --> agent_name: The name of the agent whose question the completion is a response to
<!-- -->:type<!-- --> message_type: <code>MessageType</code>
<!-- -->:param<!-- --> message_type: The type of message to be created
:rtype: <code>ChatMessage</code>
:return: A ChatMessage object containing the structured response</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>completion</strong> (<em>ChatCompletion</em>)</li>
<li><strong>agent_name</strong> (<em>str</em>)</li>
<li><strong>message_type</strong> (<em>MessageType</em>)</li>
</ul>
</li>
<li><strong>Return type:</strong>
<em>ChatMessage</em></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="send_requestbody-1">send_request(body)<a href="#send_requestbody-1" class="hash-link" aria-label="Direct link to send_request(body)" title="Direct link to send_request(body)">â€‹</a></h4>
<ul>
<li><strong>Return type:</strong>
<code>ChatCompletion</code></li>
<li><strong>Parameters:</strong>
<strong>body</strong> (<em>dict</em> *[*<em>str</em> <em>,</em> <em>Any</em> <em>]</em>)</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="truncate_chat_historymessages-n_tokens_tool_descriptions-1">truncate_chat_history(messages, n_tokens_tool_descriptions)<a href="#truncate_chat_historymessages-n_tokens_tool_descriptions-1" class="hash-link" aria-label="Direct link to truncate_chat_history(messages, n_tokens_tool_descriptions)" title="Direct link to truncate_chat_history(messages, n_tokens_tool_descriptions)">â€‹</a></h4>
<p>Truncate the chat history to fit within the maximum token limit. The token limit is calculated as follows:
We retain the first two messages i.e. system prompt and initial user prompt and the last message.
We then truncate from left, removing messages from the chat history until the total token count is within the
limit. We also account for the token count of the tool descriptions.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>messages</strong> â€“ The list of messages in the conversation</li>
<li><strong>n_tokens_tool_descriptions</strong> â€“ The number of tokens used by the tool descriptions</li>
</ul>
</li>
<li><strong>Return type:</strong>
<code>list</code>[<code>dict</code>]</li>
<li><strong>Returns:</strong>
The truncated chat history</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/agentic-diskurs/diskurs/edit/main/docs-site/docs/llm_client.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/diskurs/docs"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Welcome to Diskursâ€™s documentation!</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/diskurs/docs/multistep_agent"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Module: Multistep Agent</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#class-diskursllm_clientbaseoaiapillmclientclient-model-tokenizer-max_tokens-max_repeat3" class="table-of-contents__link toc-highlight"><em>class</em> diskurs.llm_client.BaseOaiApiLLMClient(client, model, tokenizer, max_tokens, max_repeat=3)</a></li><li><a href="#class-diskursllm_clientopenaillmclientclient-model-tokenizer-max_tokens-max_repeat3" class="table-of-contents__link toc-highlight"><em>class</em> diskurs.llm_client.OpenAILLMClient(client, model, tokenizer, max_tokens, max_repeat=3)</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/diskurs/docs">Documentation</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/agentic-diskurs/diskurs/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Issues<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/diskurs/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/agentic-diskurs/diskurs" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2024 Agentic Diskurs.</div></div></div></footer></div>
</body>
</html>